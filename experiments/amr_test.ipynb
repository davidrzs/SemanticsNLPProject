{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17384bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:527: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPretrainedModel` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from amrlib.models.parse_spring.tokenization_bart import AMRBartTokenizer\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'AMRBartTokenizer'.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BartConfig' object has no attribute 'static_position_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#copied from https://github.com/muyeby/AMRBART/blob/5461144b2e1984d324919f459028b46b335d4496/spring/spring_amr/utils.py\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43minstantiate_model_and_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Programming/SemanticsNLPProject/experiments/utils.py:72\u001B[0m, in \u001B[0;36minstantiate_model_and_tokenizer\u001B[0;34m(name, checkpoint, additional_tokens_smart_init, dropout, attention_dropout, from_pretrained, init_reverse, collapse_name_ops, penman_linearization, use_pointer_tokens, raw_graph)\u001B[0m\n\u001B[1;32m     64\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m AMRBartTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     65\u001B[0m         tokenizer_name,\n\u001B[1;32m     66\u001B[0m         collapse_name_ops\u001B[38;5;241m=\u001B[39mcollapse_name_ops,\n\u001B[1;32m     67\u001B[0m         use_pointer_tokens\u001B[38;5;241m=\u001B[39muse_pointer_tokens,\n\u001B[1;32m     68\u001B[0m         config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m     69\u001B[0m     )\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m from_pretrained:\n\u001B[0;32m---> 72\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mAMRBartForConditionalGeneration\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     model \u001B[38;5;241m=\u001B[39m AMRBartForConditionalGeneration(config)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/modeling_utils.py:1843\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   1841\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1842\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m no_init_weights(_enable\u001B[38;5;241m=\u001B[39m_fast_init):\n\u001B[0;32m-> 1843\u001B[0m         model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m from_pt:\n\u001B[1;32m   1846\u001B[0m     \u001B[38;5;66;03m# restore default dtype\u001B[39;00m\n\u001B[1;32m   1847\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Programming/SemanticsNLPProject/experiments/utils.py:683\u001B[0m, in \u001B[0;36mAMRBartForConditionalGeneration.__init__\u001B[0;34m(self, config, backpointer_idx)\u001B[0m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config: bart\u001B[38;5;241m.\u001B[39mBartConfig, backpointer_idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(config)\n\u001B[0;32m--> 683\u001B[0m     base_model \u001B[38;5;241m=\u001B[39m \u001B[43mAMRBartModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackpointer_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    684\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m base_model\n\u001B[1;32m    685\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_index \u001B[38;5;241m=\u001B[39m base_model\u001B[38;5;241m.\u001B[39mshared\u001B[38;5;241m.\u001B[39mpadding_idx\n",
      "File \u001B[0;32m~/Programming/SemanticsNLPProject/experiments/utils.py:608\u001B[0m, in \u001B[0;36mAMRBartModel.__init__\u001B[0;34m(self, config, backpointer_idx)\u001B[0m\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    606\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackpointer_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared\u001B[38;5;241m.\u001B[39mnum_embeddings \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 608\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m \u001B[43mAMRBartEncoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshared\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackpointer_idx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackpointer_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder \u001B[38;5;241m=\u001B[39m AMRBartDecoder(config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared, backpointer_idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackpointer_idx)\n\u001B[1;32m    611\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_weights()\n",
      "File \u001B[0;32m~/Programming/SemanticsNLPProject/experiments/utils.py:334\u001B[0m, in \u001B[0;36mAMRBartEncoder.__init__\u001B[0;34m(self, config, embed_tokens, backpointer_idx)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_source_positions \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mmax_position_embeddings\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_tokens \u001B[38;5;241m=\u001B[39m embed_tokens\n\u001B[0;32m--> 334\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatic_position_embeddings\u001B[49m:\n\u001B[1;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_positions \u001B[38;5;241m=\u001B[39m bart\u001B[38;5;241m.\u001B[39mSinusoidalPositionalEmbedding(\n\u001B[1;32m    336\u001B[0m         config\u001B[38;5;241m.\u001B[39mmax_position_embeddings, embed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_idx\n\u001B[1;32m    337\u001B[0m     )\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/configuration_utils.py:253\u001B[0m, in \u001B[0;36mPretrainedConfig.__getattribute__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattribute_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattribute_map\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    252\u001B[0m     key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattribute_map\u001B[39m\u001B[38;5;124m\"\u001B[39m)[key]\n\u001B[0;32m--> 253\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'BartConfig' object has no attribute 'static_position_embeddings'"
     ]
    }
   ],
   "source": [
    "#copied from https://github.com/muyeby/AMRBART/blob/5461144b2e1984d324919f459028b46b335d4496/spring/spring_amr/utils.py\n",
    "\n",
    "model, tokenizer = instantiate_model_and_tokenizer()\n",
    "\n",
    "\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b00cc792",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 8: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m batch \u001B[38;5;241m=\u001B[39m tokenizer(example_english_phrase, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m generated_ids \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m----> 6\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3265\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_decode\u001B[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3242\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_decode\u001B[39m(\n\u001B[1;32m   3243\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3244\u001B[0m     sequences: Union[List[\u001B[38;5;28mint\u001B[39m], List[List[\u001B[38;5;28mint\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3247\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   3248\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   3249\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3250\u001B[0m \u001B[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001B[39;00m\n\u001B[1;32m   3251\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3263\u001B[0m \u001B[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001B[39;00m\n\u001B[1;32m   3264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m   3266\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(\n\u001B[1;32m   3267\u001B[0m             seq,\n\u001B[1;32m   3268\u001B[0m             skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[1;32m   3269\u001B[0m             clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[1;32m   3270\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3271\u001B[0m         )\n\u001B[1;32m   3272\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m sequences\n\u001B[1;32m   3273\u001B[0m     ]\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3266\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   3242\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_decode\u001B[39m(\n\u001B[1;32m   3243\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3244\u001B[0m     sequences: Union[List[\u001B[38;5;28mint\u001B[39m], List[List[\u001B[38;5;28mint\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3247\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   3248\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   3249\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3250\u001B[0m \u001B[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001B[39;00m\n\u001B[1;32m   3251\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3263\u001B[0m \u001B[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001B[39;00m\n\u001B[1;32m   3264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   3265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m-> 3266\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3267\u001B[0m \u001B[43m            \u001B[49m\u001B[43mseq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3268\u001B[0m \u001B[43m            \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3269\u001B[0m \u001B[43m            \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3270\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3271\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3272\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m sequences\n\u001B[1;32m   3273\u001B[0m     ]\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3304\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3301\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[1;32m   3302\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[0;32m-> 3304\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3306\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3307\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3308\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3309\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/tokenization_utils.py:946\u001B[0m, in \u001B[0;36mPreTrainedTokenizer._decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m    944\u001B[0m         current_sub_text\u001B[38;5;241m.\u001B[39mappend(token)\n\u001B[1;32m    945\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m current_sub_text:\n\u001B[0;32m--> 946\u001B[0m     sub_texts\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_tokens_to_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_sub_text\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spaces_between_special_tokens:\n\u001B[1;32m    949\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(sub_texts)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/transformers/models/bart/tokenization_bart.py:305\u001B[0m, in \u001B[0;36mBartTokenizer.convert_tokens_to_string\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_tokens_to_string\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens):\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;124;03m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 305\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    306\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbyte_decoder[c] \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m text])\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors)\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "\u001B[0;31mTypeError\u001B[0m: sequence item 8: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "example_english_phrase = \"ETH requires hard work.\"\n",
    "batch = tokenizer(example_english_phrase, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(batch[\"input_ids\"])\n",
    "\n",
    "\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}