{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 56>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__str__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset)\n\u001B[0;32m---> 56\u001B[0m a \u001B[38;5;241m=\u001B[39m \u001B[43mSemanticEntailmentAMRDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcola\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mprint\u001B[39m(a)\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mSemanticEntailmentAMRDataset.__init__\u001B[0;34m(self, dataset_name, amr_parser_path)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mamr_parser \u001B[38;5;241m=\u001B[39m amrlib\u001B[38;5;241m.\u001B[39mload_stog_model(amr_parser_path)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_name \u001B[38;5;241m=\u001B[39m dataset_name\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnlp_tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dataset_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhans\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhans\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/spacy/__init__.py:30\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, **overrides)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m depr_path \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     29\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(Warnings\u001B[38;5;241m.\u001B[39mW001\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mdepr_path), \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m)\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/spacy/util.py:175\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, **overrides)\u001B[0m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(name, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexists\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# Path or Path-like to model data\u001B[39;00m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m load_model_from_path(name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides)\n\u001B[0;32m--> 175\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import amrlib\n",
    "from amrlib.alignments.rbw_aligner import RBWAligner\n",
    "from amrlib.graph_processing.annotator import add_lemmas\n",
    "import spacy\n",
    "\n",
    "class SemanticEntailmentAMRDataset:\n",
    "    def __init__(self, dataset_name, amr_parser_path='/home/david/tmp/model_parse_xfm_bart_base-v0_1_0'):\n",
    "\n",
    "        self.amr_parser = amrlib.load_stog_model(amr_parser_path)\n",
    "        self.dataset_name = dataset_name\n",
    "        self.nlp_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        if dataset_name == 'hans':\n",
    "            self.dataset = load_dataset('hans')\n",
    "        elif dataset_name in ['mnli', 'wnli', 'qnli', 'mnli_mismatched', 'mnli_matched','cola']:\n",
    "            self.dataset = load_dataset('glue', dataset_name)\n",
    "        else:\n",
    "            raise Exception(f'Dataset {dataset_name} does not exist.')\n",
    "\n",
    "    def splits(self):\n",
    "        return self.dataset.keys()\n",
    "\n",
    "    def to_amr(self, split):\n",
    "        if self.dataset_name != 'cola':\n",
    "            raise Exception(f'Dataset {self.dataset_name} not supported.')\n",
    "        sent_idx_list = [(entry['sentence'], entry['idx']) for entry in  self.dataset[split]]\n",
    "        sent_list, idx_list = list(zip(*sent_idx_list))\n",
    "        sent_list, idx_list = list(sent_list), list(idx_list)\n",
    "        # print(sent_list, idx_list)\n",
    "        parsed_sents = self.amr_parser.parse_sents(sent_list, add_metadata=True)\n",
    "        return parsed_sents, idx_list\n",
    "\n",
    "    def aligned_AMR(self, arm_graph_string):\n",
    "        pg = add_lemmas(arm_graph_string, snt_key='snt')\n",
    "        aligner = RBWAligner.from_penman_w_json(pg)\n",
    "        penman_graph = aligner.get_penman_graph()\n",
    "        return penman_graph\n",
    "\n",
    "\n",
    "    def get_alignments(self, text):\n",
    "        # returns an array where the i-th entry is a 2-tuple of the start and end characters index of the word (the last exclusive!)\n",
    "        # TODO inclusive or exclusive?\n",
    "        tokenized = self.nlp_tokenizer(text)\n",
    "        alignments = []\n",
    "        for i, token in enumerate(tokenized):\n",
    "            alignments.append((token.idx, token.idx + len(token)))\n",
    "        return alignments\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = SemanticEntailmentAMRDataset('cola')\n",
    "\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gid=x Start paren present but and is not a new concept\n",
      "gid=x Start paren present but amr-unknown is not a new concept\n",
      "gid=x Start paren present but old is not a new concept\n",
      "gid=x Start paren present but likely-01 is not a new concept\n",
      "gid=x Missing starting paren for node a3/any\n",
      "gid=x Start paren present but name is not a new concept\n",
      "gid=x Initial node constructed when triples not empty, ignoring token.\n",
      "gid=x Missing starting paren for node g2/garden\n",
      "gid=x Start paren present but person is not a new concept\n",
      "gid=x Start paren present but asparagus is not a new concept\n",
      "gid=x Start paren present but eat-01 is not a new concept\n",
      "gid=x Start paren present but name is not a new concept\n",
      "gid=x Start paren present but right-06 is not a new concept\n",
      "gid=x Start paren present but believe-01 is not a new concept\n",
      "gid=x Start paren present but play-11 is not a new concept\n",
      "gid=x Start paren present but drop-01 is not a new concept\n",
      "gid=x Start paren present but drop-01 is not a new concept\n",
      "gid=x Start paren present but have-rel-role-91 is not a new concept\n",
      "gid=x Start paren present but beg-01 is not a new concept\n",
      "gid=x Start paren present but organization is not a new concept\n",
      "gid=x Start paren present but read-01 is not a new concept\n",
      "gid=x Missing starting paren for node n/noon\n",
      "gid=x Start paren present but name is not a new concept\n",
      "gid=x Start paren present but person is not a new concept\n"
     ]
    }
   ],
   "source": [
    "ps, li = a.to_amr('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ::snt Bill whistled past the house.\\n(w / whistle-01\\n      :ARG0 (p / person\\n            :name (n / name\\n                  :op1 \"Bill\"))\\n      :path (p2 / past\\n            :op1 (h / house)))'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Input \u001B[0;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mamrlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgraph_processing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mannotator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m add_lemmas\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m penman_graph \u001B[38;5;241m=\u001B[39m add_lemmas(ps[\u001B[38;5;241m0\u001B[39m], snt_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msnt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/spacy/__init__.py:30\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, **overrides)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m depr_path \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     29\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(Warnings\u001B[38;5;241m.\u001B[39mW001\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mdepr_path), \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m)\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moverrides\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/SemanticsNLPProject-2N7QzmHs/lib/python3.9/site-packages/spacy/util.py:163\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, **overrides)\u001B[0m\n\u001B[1;32m    161\u001B[0m data_path \u001B[38;5;241m=\u001B[39m get_data_path()\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data_path\u001B[38;5;241m.\u001B[39mexists():\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE049\u001B[38;5;241m.\u001B[39mformat(path\u001B[38;5;241m=\u001B[39mpath2str(data_path)))\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(name, basestring_):  \u001B[38;5;66;03m# in data dir / shortcut\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblank:\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# shortcut for blank model\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: [E049] Can't find spaCy data directory: 'None'. Check your installation and permissions, or use spacy.util.set_data_path to customise the location if necessary."
     ]
    }
   ],
   "source": [
    "from amrlib.graph_processing.annotator import add_lemmas\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "penman_graph = add_lemmas(ps[0], snt_key='snt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from amrlib.alignments.rbw_aligner import RBWAligner\n",
    "aligner = RBWAligner.from_string_w_json(ps[0])  # use this with a graph string that is properly annotated\n",
    "penman_graph = aligner.get_penman_graph()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}